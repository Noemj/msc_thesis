-Most organizations have many ideas, but the return-on-investment (ROI) for many may be unclear and the evaluation itself may be expensive.

-When a company builds a system for experimentation, the cost of testing and experimental failure becomes small, thus encouraging innovation through experimentation

-Implementing an experiment on a website involves two
components. The first component is the randomization algorithm,
which is a function that maps users to variants. The second
component is the assignment method, which uses the output of the
randomization algorithm to determine the experience that each
user will see on the website. During the experiment, observations
must be collected, and data needs to be aggregated and analyzed

-Traffic splitting is a method that involves implementing each
variant of an experiment on a different fleet of servers, be it
physical or virtual.

-An alternative method is server-side selection, whereby API calls
embedded into the website’s servers invoke the randomization
algorithm and enable branching logic that produces a different
user experience for each variant.

-A final alternative is client-side selection, whereby JavaScript
calls embedded into each web page contact a remote service for
assignments and dynamically modify the page to produce the
appropriate user experience. Client-side experiments are
generally easier to implement than server-side experiments
because the developer need only embed canned snippets of
JavaScript into each page. However, this method severely limits
the features that may be subject to experimentation; in particular,
experiments on dynamic content or backend features are much
harder to implement.

-In order to compare metrics across experiment variants, a website must first record the
treatment assignments of all end users who visit the site during an experiment. Then,
the website must collect raw data such as page views, clicks, revenue, render time, or
customer-feedback selections. Each row of this raw data must be annotated with the
identifier of the variant of each experiment that the user saw on the page request.

-The system must then convert this raw data into metrics—numerical summaries that can
be compared between variants of an experiment to determine the outcome. Metrics
can range from simple aggregates (total page views) all the way to complex inferred
measures (customer satisfaction or search relevance). To compute metrics, the system
applies basic transformations and then aggregates the observations, grouping by
experiment, variant, and any other dimensions that the experimenter wishes to analyze
(for example, demographics or user agent). Additional transformations may be
applied at this point to produce more complex measures. From here, we create a table
of metric values, broken down by dimensions, experiment, and (most importantly)
variant.We can now compare metric values between variants and determine statistical
significance using either any of a number of statistical tests.

-most existing data collection systems
are not designed for the statistical analyses that are required to correctly analyze the
results of a controlled experiment

-Service-based collection Under this model, the website implements a service
specifically designed to record and store observation data. Service calls may be
placed in a number of locations, including web servers, application servers, backend
algorithm services, and even the end user’s browser (called via JavaScript). 