\section{Needs, problems and challenges}

\subsection{Problems} %Should address an underlying research question

TODO:
Research problem: "High-level slogan"
How to adopt CD and CE in the case company?

Research questions:

Larger context, narrow context
Subproblems
More concrete (Who is having a problem?)

-Continuous experimentation is a model, not a process: what, when, who, how

-Collecting feedback in B2B domain
  In B2B domain the connection with end users might not be established properly, and the feedback can only be received from the other company's key members. However, the feedback from end users is especially important. 

  Without the data from end users, no usage data is collected all. 

  Pilot approach: Prioritize bugs based on the amount of occurrences

-Collecting feedback without a SaaS product
  In a SaaS platform it is easier to plug in measurement tools to the software, as the environment is that of the company responsible for the service. Without a SaaS environment, the data has to be collected and stored in customer environment. 

  Thus, the collection of the data takes unnecessary effort, and the whole usage data is totally missing. The product development decisions aren't therefore based on data, and are potentially less effective than decisions based on measurable, quantitative data.

-The time between the idea and implementation is too long
  Currently the ideas are not tested at all, but the effects are planned in a waterfall style and implemented thoroughly before a release. Making partial implementations could however allow the use of quantitative or qualitative data to measure whether an idea is worth implementing at all.

  Management can also better convince other internal stakeholders that a feature might be of importance

-Time dimension. If an impact is done right now, when will the effects be visible?
  Adding the correct measurement metrics to a feature also make the impact more visible.

-Conventions for production deployment. The whole pipe between the idea and implementation isn't yet completely perceived. Some parts are visible, but the entirety is still unclear.
  Forming the development pipeline to include both experiments and automated deployment makes the entirety clear from the idea to the release.

-How do we validate whether a feature is succesful or not? In B2C this is usually done by observing shifts in sales, but in our product there's no such thing.
  The measurement metric has to be chosen such that a decision can be made to judge if the feature is succesful or not.

-How to use the collected data to improve decision making?
  TODO

-Trello flow time?
  Occasional deploying takes time. Speeding it up would eventually improve flow time.

-Possible thing we can analyze is user experience, but is it enough?

\subsection{goals}
Requirements should be testable

-Continuously collect feedback from customers.
    Subgoal: Define the type of feedback to be collected
    Subgoal: Define a mechanism for feedback collection
    Subgoal: figure out how to collect feedback
    Subgoal: figure who is responsible for analyzing feedback
	Req: Feedback can be continuously collected from customers
	Req: The person responsible for collecting feedback can be identified 
-Reduce the length of the delivery cycle.
	Subgoal: Identify the components in the delivery cycle
	Subgoal: Identify what takes too long
	Req: The delivery cycle is completely perceived
	Req: The length of the delivery from idea to deployment is shortened
-Be able to measure the effectiveness of a feature.
	Subgoal: Implement measurement tools
	Subgoal: Implement data analysis tools
	Subgoal: collect data whenever a feature is deployed to measure the performance	
    Req: Data can be collected from an implemented feature 
    Req: Collected data can be analyzed to form an answer

\subsection{solutions}

\subsection{solution idea}