##Objective—what to achieve?##




The study is an exploratory case study, which attempts to figure out how continuous deployment and continuous experimentation can be integrated to the development process of the company in question. The study specifically aims to identify the main problems and key success factors with regards to these approaches. This includes analyzing the current development process, seeking the current problems faced in the development process and understanding the requirements of continuous deployment and continuous experimentation. 

##The case—what is studied?##

Kitchenham et al. (Kitchenham et al., 2002) state that the experimental context needs the three elements: background information, discussion of research hypotheses, and information about related research. The two former will be discussed here, and the latter in the section "Frame of reference". 

The company in question is Steeri Oy, which is a medium-sized company specializing in managing, analyzing and improving the usage of customer data. In this research, the unit under the study is the Development & Integration team, which is split into two sub-teams: Dialog and CDM. The Dialog team focuses on developing a marketing automation, which .... The CDM team focuses on building a Master Data Management (cite) solution, which ..... 

The organisation of Steeri Oy is of a divisional type, with each business area forming independent teams based on the products and projects. The sub-teams under study have a common team leader, but different product owners and middle management.  

Under analysis is also the company's ways to interact with the customer.

##Theory—frame of reference##

Based on the existing research in Continuous Experimentation and Continuous Deployment:

Crook et al: Seven pitfalls to avoid when running controlled experiments on the web
Eric Ries: The lean startup: How today's entrepreneurs use continuous innovation to create radically successful businesses
Kohavi et al: Practical guide to controlled experiments on the web: listen to your customers not to the hippo
Fagerholm et al: Building Blocks for Continuous Experimentation
Jan Bosch: Building products as innovation experiment systems
Olsson et al: Climbing the" Stairway to Heaven"--A Mulitiple-Case Study Exploring Barriers in the Transition from Agile Development towards Continuous Deployment of Software
Humble et al: The deployment production line
Humble et al: Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation
Microsoft EXP platform

The researcher is a part of the development team of the company, but the viewpoint ...

##Research questions—what to know? these fulfill the objective##

How to collect feedback in a B2B domain?
	subquestion: Who should use the collected feedback?
	subquestion: Who is responsible for collecting the feedback?

How to collect feedback without a SaaS product?


How to shorten the time between the idea and implementation?
If an impact is done right now, when will the effects be visible?
The whole pipe between the idea and implementation isn't yet completely perceived. Some parts are visible, but the entirety is still unclear.
How do we validate whether a feature is succesful or not? 

##Methods—how to collect data?##

The data is collected from interviews, documents and Trello.

##Selection strategy—where to seek data?##

Interview data will be primarily sought from the developers of the development teams and the managers of the team.





1. What is the case and its units of analysis?
2. Are clear objectives, preliminary research questions, hypotheses (if any) defined in advance?
3. Is the theoretical basis—relation to existing literature or other cases—defined?
4. Are the authors’ intentions with the research made clear?
5. Is the case adequately defined (size, domain, process, subjects…)?
6. Is a cause–effect relation under study? If yes, is it possible to distinguish the cause from other factors using
the proposed design?
7. Does the design involve data from multiple sources (data triangulation), using multiple methods (method
triangulation)?
8. Is there a rationale behind the selection of subjects, roles, artifacts, viewpoints, etc.?
9. Is the specified case relevant to validly address the research questions (construct validity)?
10. Is the integrity of individuals/organizations taken into account?



What is the object of study? 
2. Is a clear purpose/objective/research question 
/hypothesis/proposition defined upfront? 
3. Is the theoretical basis - relation to existing literature 
and other cases - defined? 
4. Are the authors’ intentions with the research made 
clear? 
5. Is the case adequately defined (size, domain, 
process…)? 
6. Is a cause-effect relation under study? If yes, is the 
cause distinguished from other factors? 
7. Will data be collected from multiple sources? Using 
multiple methods? 
8. Is there a rationale behind the selection of roles, 
artefacts, viewpoints, etc.? 
9. Are the case study settings relevant to validly address 
for the research question? 
10. Is the integrity of individuals/organizations taken into 
account? 
Preparation for Data Collection 
11. Is a protocol for data collection and analysis derived 
(what, why, how)? 
12. Are multiple data sources and collection methods 
planned? 
13. For quantitative data, are the measurements well 
defined? 
14. Are the planned methods and measurements sufficient 
to fulfil the objective of the study? 
15. Is the study design approved by a review board, and has 
informed consent obtained from individuals and 
organizations? 
Collecting Evidence 
16. Are data collected according to the protocol? 
17. Is the observed phenomenon correctly implemented 
(e.g. to what extent is a design method under study 
actually used)? 
18. Are data recorded to enable further analysis? 
19. Are sensitive results identified (for individuals, 
organization or project)? 
20. Are the data collection procedures well traceable? 
21. Do the collected data provide ability to address the 
research question? 
Analysis of Collected Data 
22. Is the analysis methodology defined, including roles and 
review procedures? 
23. Is a chain of evidence shown with traceable inferences 
from data to research questions and existing theory? 
24. Are alternative perspectives and explanations used in 
the analysis? 
25. Is a cause-effect relation under study? If yes, is the 
cause distinguished from other factors? 
26. Are there clear conclusions from the analysis, including 
recommendations for practice/further research? 
27. Are threats to validity addressed in a systematic way? 
 
 
 
Reporting 
28. Are the case and its context adequately reported? 
29. Are the research questions and corresponding answers 
reported? 
30. Are related theory, hypotheses and propositions clearly 
reported? 
31. Are the data collection procedures presented, with 
relevant motivation? 
32. Are sufficient raw data presented? 
33. Are the analysis procedures clearly reported. 
34. Are threats to validity analyses reported? 
35. Are ethical issues reported openly (personal intentions, 
integrity issues) 
36. Does the report contain conclusions, implications for 
practice and future research? 
37. Does the report give a realistic and credible impression? 
38. Is the report suitable for its audience, easy to read and 
well structured? 