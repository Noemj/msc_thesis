Case study design:

1. Case study design: objectives are defined and the case study is planned.
2. Preparation for data collection: procedures and protocols for data collection are defined.138
3. Collecting evidence: execution with data collection on the studied case.
4. Analysis of collected data
5. Reporting

Case study plan:

Objective—what to achieve?

The objective of the study may be, for example, exploratory, descriptive, explanatory, or
improving. The objective is naturally more generally formulated and less precise than in
fixed research designs. The objective is initially more like a focus point which evolves
during the study. The research questions state what is needed to know in order to fulfill the
objective of the study. Similar to the objective, the research questions evolve during the
study and are narrowed to specific research questions during the study iterations

& The case—what is studied?

The case may in general be virtually anything which is a “contemporary phenomenon in
its real-life context” (Yin 2003). In software engineering, the case may be a software
development project, which is the most straightforward choice. 

& Theory—frame of reference

Defining the frame of reference of the study makes the
context of the case study research clear, and helps both those conducting the research and
those reviewing the results of it.

& Research questions—what to know?


& Methods—how to collect data?

The principal decisions on methods for data collection are defined at design time for the
case study, although detailed decisions on data collection procedures are taken later.
Lethbridge et al. (2005) define three categories of methods: direct (e.g. interviews), indirect
(e.g. tool instrumentation) and independent (e.g. documentation analysis).

& Selection strategy—where to seek data?


on case studies:

Case studies do not generate the same results on e.g. causal relationships as
controlled experiments do, but they provide deeper understanding of the phenomena under
study. As they are different from analytical and controlled empirical studies, case studies
have been criticized for being of less value, impossible to generalize from, being biased by
researchers etc. This critique can be met by applying proper research methodology practices
as well as reconsidering that knowledge is more than statistical significance (Flyvbjerg
2007; Lee 1989).

Finally, a case study may contain elements of other research methods, e.g. a survey may
be conducted within a case study, literature search often precede a case study and archival
analyses may be a part of its data collection. Ethnographic methods, like interviews and
observations are mostly used for data collection in case studies.

A positivist case study searches
evidence for formal propositions, measures variables, tests hypotheses and draws inferences
from a sample to a stated population, i.e. is close to the natural science research model (Lee
1989) and related to Robson’s explanatory category. A critical case study aims at social
critique and at being emancipatory, i.e. identifying different forms of social, cultural and
political domination that may hinder human ability. Improving case studies may have a
character of being critical. An interpretive case study attempts to understand phenomena
through the participants’ interpretation of their context, which is similar to Robson’s
exploratory and descriptive types. Software engineering case studies tend to lean towards a
positivist perspective, especially for explanatory type studies

Conducting research on real world issues implies a trade-off between level of control
and degree of realism. The realistic situation is often complex and non-deterministic, which
hinders the understanding of what is happening, especially for studies with explanatory
purposes. On the other hand, increasing the control reduces the degree of realism,
sometimes leading to the real influential factors being set outside the scope of the study.

In a fixed design process, all parameters are defined at
the launch of the study, while in a flexible design process key parameters of the study may be
changed during the course of the study. Case studies are typically flexible design studies, while
experiments and surveys are fixed design studies.

Triangulation is important to increase the precision of empirical research. Triangulation
means taking different angles towards the studied object and thus providing a broader
picture. The need for triangulation is obvious when relying primarily on qualitative data,
which is broader and richer, but less precise than quantitative data. However, it is relevant
also for quantitative data, e.g. to compensate for measurement or modeling errors. Four
different types of triangulation may be applied (Stake 1995):
Data (source) triangulation—using more than one data source or collecting the same
data at different occasions.
Observer triangulation—using more than one observer in the study.
Methodological triangulation—combining different types of data collection methods,
e.g. qualitative and quantitative methods.
Theory triangulation—using alternative theories or viewpoints.

Has research questions set out from the beginning of the study
Data is collected in a planned and consistent manner
Inferences are made from the data to answer the research question
Explores a phenomenon, or produces an explanation, description, or causal analysis of it
Threats to validity are addressed in a systematic way."

The research questions state what is needed to know in order to fulfill the
objective of the study. Similar to the objective, the research questions evolve during the
study and are narrowed to specific research questions during the study iterations

CASE STUDY CHECkLIST

1. What is the case and its units of analysis?
2. Are clear objectives, preliminary research questions, hypotheses (if any) defined in advance?
3. Is the theoretical basis—relation to existing literature or other cases—defined?
4. Are the authors’ intentions with the research made clear?
5. Is the case adequately defined (size, domain, process, subjects…)?
6. Is a cause–effect relation under study? If yes, is it possible to distinguish the cause from other factors using
the proposed design?
7. Does the design involve data from multiple sources (data triangulation), using multiple methods (method
triangulation)?
8. Is there a rationale behind the selection of subjects, roles, artifacts, viewpoints, etc.?
9. Is the specified case relevant to validly address the research questions (construct validity)?
10. Is the integrity of individuals/organizations taken into account?


REST

1. Case study design: objectives are defined and 
the case study is planned. 
2. Preparation for data collection: procedures and 
protocols for data collection are defined. 
3. Collecting evidence: execution with data 
collection on the studied case. 
4. Analysis of collected data 
5. Reporting 

Preparation for Data Collection 
11. Is a protocol for data collection and analysis derived 
(what, why, how)? 
12. Are multiple data sources and collection methods 
planned? 
13. For quantitative data, are the measurements well 
defined? 
14. Are the planned methods and measurements sufficient 
to fulfil the objective of the study? 
15. Is the study design approved by a review board, and has 
informed consent obtained from individuals and 
organizations? 
Collecting Evidence 
16. Are data collected according to the protocol? 
17. Is the observed phenomenon correctly implemented 
(e.g. to what extent is a design method under study 
actually used)? 
18. Are data recorded to enable further analysis? 
19. Are sensitive results identified (for individuals, 
organization or project)? 
20. Are the data collection procedures well traceable? 
21. Do the collected data provide ability to address the 
research question? 
Analysis of Collected Data 
22. Is the analysis methodology defined, including roles and 
review procedures? 
23. Is a chain of evidence shown with traceable inferences 
from data to research questions and existing theory? 
24. Are alternative perspectives and explanations used in 
the analysis? 
25. Is a cause-effect relation under study? If yes, is the 
cause distinguished from other factors? 
26. Are there clear conclusions from the analysis, including 
recommendations for practice/further research? 
27. Are threats to validity addressed in a systematic way? 
 
 
 
Reporting 
28. Are the case and its context adequately reported? 
29. Are the research questions and corresponding answers 
reported? 
30. Are related theory, hypotheses and propositions clearly 
reported? 
31. Are the data collection procedures presented, with 
relevant motivation? 
32. Are sufficient raw data presented? 
33. Are the analysis procedures clearly reported. 
34. Are threats to validity analyses reported? 
35. Are ethical issues reported openly (personal intentions, 
integrity issues) 
36. Does the report contain conclusions, implications for 
practice and future research? 
37. Does the report give a realistic and credible impression? 
38. Is the report suitable for its audience, easy to read and 
well structured? 
 
 
Appendix B. Reviewer’s Checklist1
 
1. Are the research questions, objects of study and case 
study context well defined? 1, 2, 5, 28, 29 
2. Is it motivated that the case is suitable to address the 
research questions? 8, 9, 14 
3. Are the hypotheses and propositions clear and relevant? 
2, 30 
4. Are the data collection procedures sufficient for the 
purpose (data sources, collection, storage, validation)? 
11, 13, 16, 18, 21, 32 
5. Are sufficient raw data presented to provide 
understanding of the case? 31 
6. Are the analysis procedures sufficient for the purpose 
(repeatable, transparent)? 22, 33 
7. Is the case study based on theory and linked to existing 
literature? 3 
8. Is a clear chain of evidence established from 
observations to conclusions? 6, 17, 20, 23, 25 
9. Are threats to validity analyses addressed in a 
systematic way? 27, 34, 37 
10. Are different views taken on the case (multiple 
collection and analysis methods, multiple authors)? 7, 
12, 22, 24 
11. Are ethical issues addressed properly (personal 
intentions, integrity issues, consent, review board 
approval)? 4, 10, 15, 19, 35 
12. Are conclusions, implications for practice and future 
research, reported suitably for its audience? 26, 29, 36,